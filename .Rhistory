#covhat = invXX * sigmahat2  ### covariance for hat theta
#return(list(theta = thetahat, covhat = covhat, sigmahat = sqrt(sigmahat2)))  ### return the result
return(list(theta = thetahat))
}
####users only need to enter the following parameters to generate the desired simulation data
N=50  #numbers of nodes
alpha=1.2  #parameter of power-law
Time=30   #The length of time
gamma0=c(-0.5,0.3,0.8,0,0) #fixed parameter of Z
beta0=rep(0.3,N) #Intercept term coefficient
Beta=c(0.1,0.5)  #Regression coefficient
Ysim=simudata(beta0, Beta, gamma0, Time, sig = 1, N, alpha)
est(Ysim,N,alpha,p)
devtools::build_vignettes()
.Last.error.trace
devtools::document()
rm(list = c("est", "simudata", "statGamma", "statMu", "W", "Y", "Z"))
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::build_vignettes()
.Last.error.trace to see where the error occurred
.Last.error.trace
devtools::build_vignettes()
.Last.error.trace
devtools::build_vignettes()
.Last.error.trace
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
devtools::build_vignettes()
devtools::build_vignettes()
.Last.error.trace
knitr::opts_chunk$set(echo = TRUE)
Z<-function(n){
x<-seq(-7,7,0.01)
truth<-dnorm(x,0,2*sqrt(2))
plot(density(rnorm(n,0,2)+rnorm(n,0,2)),main="density estimate of the normal addition model",ylim=c(0,0.25),lwd=2,lty=2)
lines(x,truth,col="red",lwd=2)
legend("topright",c("true","estimated"),col=c("red","black"),lwd=2,lty=c(1,2))
}
Z(10000)
library(xtable)
library(gt)
library(datasets)
library(knitr)
data(airquality)
table<-kable(airquality[1:7,])
table
gt_table<-gt(data=airquality[1:7,])
gt_table <-
gt_table %>%
tab_header(
title=md("**New York Air Quality Measurements**"),
subtitle=md("*The first seven days of May, 1973*")
) %>%
tab_source_note(
source_note="Source: The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data)."
) %>%
tab_source_note(
source_note=md("Reference: Chambers, J. M., Cleveland, W. S., Kleiner, B. and Tukey, P. A. (1983) *Graphical Methods for Data Analysis*. Belmont, CA: Wadsworth.")
)
gt_table
n<-1e5;
rleigh<-function(sigma){
u<-runif(n)
x<-sqrt(-2*(sigma^2)*log(1-u))
y<-seq(0,100,0.1)
hist(x,probability = TRUE,main=paste ("sigma=",as.character(sigma),sep=""),border=NA)
lines(y,(y/(sigma^2))*exp(-y^2/(2*sigma^2)))
}
par(mfrow=c(2,3))
rleigh(4)
rleigh(5)
rleigh(10)
rleigh(15)
rleigh(20)
n<-1e5;
rleigh<-function(sigma){
u<-runif(n)
x<-sqrt(-2*(sigma^2)*log(1-u))
y<-seq(0,100,0.1)
hist(x,probability = TRUE,main=paste ("sigma=",as.character(sigma),sep=""),border=NA)
lines(y,(y/(sigma^2))*exp(-y^2/(2*sigma^2)))
}
par(mfrow=c(2,3))
rleigh(4)
rleigh(5)
rleigh(10)
rleigh(15)
rleigh(20)
n<-1000
x1<-rnorm(n,0,1)
x2<-rnorm(n,3,1)
r1<-sample(c(1,0),n,replace=TRUE,prob=c(0.75,0.25))
z1<-r1*x1+(1-r1)*x2
r2<-sample(c(1,0),n,replace=TRUE,prob=c(0.6,0.4))
z2<-r2*x1+(1-r2)*x2
r3<-sample(c(1,0),n,replace=TRUE,prob=c(0.5,0.5))
z3<-r3*x1+(1-r3)*x2
r4<-sample(c(1,0),n,replace=TRUE,prob=c(0.4,0.6))
z4<-r4*x1+(1-r4)*x2
r5<-sample(c(1,0),n,replace=TRUE,prob=c(0.1,0.9))
z5<-r5*x1+(1-r5)*x2
par(mfrow=c(2,3))
hist(z1,prob=TRUE,main=expression(p==0.75));
hist(z2,prob=TRUE,main=expression(p==0.6));
hist(z3,prob=TRUE,main=expression(p==0.5));
hist(z4,prob=TRUE,main=expression(p==0.4));
hist(z5,prob=TRUE,main=expression(p==0.1));
n<-1e4;alpha<-4;beta<-3;lambda<-5;t<-10
x<-numeric(n)
for (i in 1:n){
Nt<-rpois(1,lambda*t)
y<-rgamma(Nt,alpha,beta)
x[i]<-sum(y)
}
meanm<-mean(x)
varm<-var(x)
meanr<-lambda*t*(alpha/beta)
varr<-lambda*t*(alpha*(1+alpha)/beta^2)
meanm
varm
meanr
varr
n<-1e4;alpha<-4;beta<-3;lambda<-5;t<-10
x<-numeric(n)
for (i in 1:n){
Nt<-rpois(1,lambda*t)
y<-rgamma(Nt,alpha,beta)
x[i]<-sum(y)
}
meanm<-mean(x)
varm<-var(x)
meanr<-lambda*t*(alpha/beta)
varr<-lambda*t*(alpha*(1+alpha)/beta^2)
meanm
varm
meanr
varr
set.seed(0)
#set parameters
x<-seq(0.1,0.9,0.1)
n<-1e4
alpha<-3
beta<-3
F1<-numeric(length(x))
B<-factorial(alpha+beta-1)/(factorial(alpha-1)*factorial(beta-1))
#function to caculate MC Integration
F<-function(t){
y<-runif(n,0,t)
m<-B*t*mean(y^(alpha-1)*(1-y)^(beta-1))
}
for (i in 1:length(x)){
F1[i]<-F(x[i])
}
plot(x,F1,type='l',xlab='x',ylab='F(x)',col=1,lwd=2)
lines(x,pbeta(x,alpha,beta),lty=2,col=2)
exbeta<-c(expression(paste("Monte Carlo Estimate")),expression(paste("pbeta function in R")))
legend("bottomright",exbeta,lty=c(1,2),col=c(1,2),lwd=2)
set.seed(0)
n<-1e4
x<-2
sig<-4 #Here we set the upper limit x=2, parameter sigma=4. You can choose any value of these two parameters.
u<-runif(n,0,x)
ua<-c(u[1:(n/2)])#use half of u to construct antithetic variable
mcm<-x*mean(u*exp(-u^2/2*sig^2)/sig^2)
mcv<-var(u*exp(-u^2/2*sig^2)/sig^2)/n
antimcm<-x*sum(ua*exp(-ua^2/2*sig^2)/sig^2+(x-ua)*exp(-(x-ua)^2/2*sig^2)/sig^2)/n
antimcv<-var(ua*exp(-ua^2/2*sig^2)/sig^2+(x-ua)*exp(-(x-ua)^2/2*sig^2)/sig^2)/(2*n)
cat("The percent reduction in variance is",antimcv/mcv)
x<-seq(1,10,0.1)
# we also tried x<-seq(1,100,1), but the g(x) close to 0 as x increase
w<-2
g<-x^2*exp(-x^2/2)/sqrt(2*pi)
f1<-sqrt(exp(1))*x^2*exp(-x/2)/10
f2<-sqrt(exp(1))*x*exp(-x^2/2)
#figure (a)
plot(x,g,type = "l",main = "(a)",ylab="",col=1,lwd = w)
lines(x,g/g,lty = 2,col=2,lwd = w)
lines(x,f1,lty = 3,col=3,lwd = w)
lines(x,f2,lty = 4,col=4,lwd = w)
legend("topright",legend=c("g","g/g","f1","f2"),lty=c(1:4),col=c(1:4),lwd = w,inset = 0.02)
#figure (b)
plot(x,g/g,type = "l",main = "(b)",lty=2,col=2,ylab="",lwd = w)
lines(x,g/f1,lty = 3,col=3,lwd = w)
lines(x,g/f2,lty = 4,col=4,lwd = w)
legend("topright",legend =c("g/g","g/f1","g/f2"),lty=c(2:4),col=c(2:4),lwd = w, inset = 0.02)
set.seed(0)
n<-1e4
u<-runif(n)
x<-sqrt(-2*log(1-u*(1-1/sqrt(exp(1)))))#inverse method
EX<-mean(x)*(1-1/sqrt(exp(1)))/sqrt(2*pi)
cat("The estimating integral is",1/2-EX)
set.seed(1)
n<-20
m<-1000
s<-0
alpha<-0.05
for (i in 1:m){
x<-rchisq(n,df=2)
y1<-mean(x)-qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
y2<-mean(x)+qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
if (y1<2 & y2>2) {
s<-s+1
}
}
cat("The coverage probability of the t-interval is",s/m)
set.seed(1)
n<-20
m<-1000
s<-0
alpha<-0.05
mu0<-1
p<-numeric(m)
for (i in 1:m){
x<-rchisq(n,df=1)
y1<-mean(x)-qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
y2<-mean(x)+qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
if (y1<1 & y2>1) {
s<-s+1
}
ttest<-t.test(x,mu=mu0)
p[i]<-ttest$p.value
}
p.hat<-mean(p<alpha)
se.hat<-round(sqrt(p.hat*(1-p.hat)/m),3)
cat("The coverage probability of the t-interval is",s/m,". The p-value is",p.hat,". And the standard error of the estimate is approximately",se.hat,".")
set.seed(1)
n<-20
m<-1000
s<-0
alpha<-0.05
mu0<-1
p<-numeric(m)
for (i in 1:m){
x<-runif(n,0,2)
y1<-mean(x)-qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
y2<-mean(x)+qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
if (y1<1 & y2>1) {
s<-s+1
}
ttest<-t.test(x,mu=mu0)
p[i]<-ttest$p.value
}
p.hat<-mean(p<alpha)
se.hat<-round(sqrt(p.hat*(1-p.hat)/m),3)
cat("The coverage probability of the t-interval is",s/m,". The p-value is",p.hat,". And the standard error of the estimate is approximately",se.hat,".")
set.seed(1)
n<-20
m<-1000
s<-0
alpha<-0.05
mu0<-1
p<-numeric(m)
for (i in 1:m){
x<-rexp(n,1)
y1<-mean(x)-qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
y2<-mean(x)+qt(1-alpha/2, df=n-1)*sd(x)/sqrt(n)
if (y1<1 & y2>1) {
s<-s+1
}
ttest<-t.test(x,mu=mu0)
p[i]<-ttest$p.value
}
p.hat<-mean(p<alpha)
se.hat<-round(sqrt(p.hat*(1-p.hat)/m),3)
cat("The coverage probability of the t-interval is",s/m,". The p-value is",p.hat,". And the standard error of the estimate is approximately",se.hat,".")
library(MASS)
set.seed(123)
d <- 3 #dimension
m <- 100 #sample sizes of A
n <- 20  #sample sizes of X in each A
sktests <- numeric(m)
cv <- qchisq(.95, d*(d+1)*(d+2)/6) #crit. values for each n
skn<-function(x){
xbar <-  apply(x,1,mean)
y <- matrix(nrow = d, ncol = n)
s <- matrix(nrow = 1, ncol = n)
for (a in 1:n){
y[,a] <- x[,a] - xbar
}
cov <- cov(t(x))
for (b in 1:n){
s[b] <- 0
for (c in 1:n){
s[b] <- s[b]+(t(y[,b])%*%ginv(cov)%*%y[,c])^3
}
}
return(sum(s)/n/6)
}
for (k in 1:m) {
x<-matrix(rnorm(d*n), nrow = d, ncol = n)
#test decision is 1 (reject) or 0
sktests[k] <- as.integer(skn(x) >= cv)
}
p.reject <- mean(sktests) #proportion rejected
p.reject
result<-matrix(nrow = 1, ncol = 4)
result[1,] <- c(0.000,0.000,0.030,0.060)
dimnames(result)[[2]] <- c("n=10","n=20","n=50","n=100")
knitr::kable(result)
library(boot); library(bootstrap); library(MASS);
set.seed(12345)
b.pca <- function(x,i){
lam <- eigen(cov(x[i,]))$values
theta_hat <- lam[1] / sum(lam)
}
x <- cbind(scor$mec, scor$vec, scor$alg, scor$ana, scor$sta)
obj <- boot(x,statistic = b.pca, R = 2000)
round(c(original=obj$t0,bias.boot=mean(obj$t)-obj$t0,se.boot=sd(obj$t)),3)
n<-nrow(scor)
theta_hat <- b.pca(x,1:n)
theta_jack <- numeric(n)
for(i in 1:n){
theta_jack[i] <- b.pca(x,(1:n)[-i])
}
bias.jack <- (n-1)*(mean(theta_jack)-theta_hat)
se.jack <- sqrt((n-1)*mean((theta_jack-theta_hat)^2))
round(c(original=theta_hat,bias.jack=bias.jack,se.jack=se.jack),3)
#normal populations (skewness=0)
library(boot);library(moments)
set.seed(12345)
skew<-0;n<-20;m<-1e4;
ci.norm<-ci.basic<-ci.perc<-matrix(NA,m,2)
boot.sk <- function(x,i){
skewness(x[i])
}
#bootstrap CI
for(i in 1:m){
U <- rnorm(n,0,1)
de <- boot(data=U , statistic=boot.sk, R = 500)
ci <- boot.ci(de , conf = 0.95 , type=c("norm","basic","perc"))
ci.norm[i,] <- ci$norm[2:3]
ci.basic[i,] <- ci$basic[4:5]
ci.perc[i,] <- ci$percent[4:5]
}
#coverage probabilities
cat('norm =',mean(ci.norm[,1]<=skew & ci.norm[,2]>=skew),
'basic =',mean(ci.basic[,1]<=skew & ci.basic[,2]>=skew),
'perc =',mean(ci.perc[,1]<=skew & ci.perc[,2]>=skew))
#miss on the left
cat('norm left=',mean(ci.norm[,1]>skew),
'basic left=',mean(ci.basic[,1]>skew),
'perc left=',mean(ci.perc[,1]>skew))
#miss on the right
cat('norm right=',mean(ci.norm[,2]<skew),
'basic right=',mean(ci.basic[,2]<skew),
'perc right=',mean(ci.perc[,2]<skew))
set.seed(12345);
n1 <- n2 <- 50;
K <- 1:100;
m <- 999;
reps <- numeric(m);
x <- rchisq(n1,2);
y <- rgamma(n2,0.5,7);
plot(y~x,main="scatter diagram",xlab="X",ylab="y")
z <- c(x,y) #sample pool
s0 <- cor.test(x,y, method ="spearman",continuity=TRUE,conf.level=0.95)$statistic
for (i in 1:m) {
k <- sample(K, size = n1, replace = FALSE)
x1 <- z[k];y1 <- z[-k] #complement of x1
reps[i] <- cor.test(x1, y1, method ="spearman",continuity=TRUE,conf.level=0.95)$statistic
}
p <- mean(abs(c(s0, reps)) >= abs(s0))
round(c(p,cor.test(x,y,method ="spearman",continuity=TRUE,conf.level=0.95)$p.value),3)
set.seed(123)
m <- 1e2; k <- 3; p <- 2;
n1 <- n2 <- 50; R <- 999; n <- n1+n2; N = c(n1,n2)
#KNN
Tn3 <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 <= n1); i2 <- sum(block2 > n1)
(i1 + i2) / (k * n)
}
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data = z, statistic = Tn3, R = R, sim = "permutation", sizes = sizes, k = k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts >= ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p);
y <- cbind(rnorm(n2),rnorm(n2));
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12)$p.value
}
library(boot)
library(energy)
library(Ball)
library(RANN)
set.seed(12345);
n1 <- n2 <- 50;
K <- 1:100;
m <- 999;
reps <- numeric(m);
x <- rchisq(n1,2);
y <- rgamma(n2,0.5,7);
plot(y~x,main="scatter diagram",xlab="X",ylab="y")
z <- c(x,y) #sample pool
s0 <- cor.test(x,y, method ="spearman",continuity=TRUE,conf.level=0.95)$statistic
for (i in 1:m) {
k <- sample(K, size = n1, replace = FALSE)
x1 <- z[k];y1 <- z[-k] #complement of x1
reps[i] <- cor.test(x1, y1, method ="spearman",continuity=TRUE,conf.level=0.95)$statistic
}
p <- mean(abs(c(s0, reps)) >= abs(s0))
round(c(p,cor.test(x,y,method ="spearman",continuity=TRUE,conf.level=0.95)$p.value),3)
set.seed(123)
m <- 1e2; k <- 3; p <- 2;
n1 <- n2 <- 50; R <- 999; n <- n1+n2; N = c(n1,n2)
#KNN
Tn3 <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 <= n1); i2 <- sum(block2 > n1)
(i1 + i2) / (k * n)
}
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data = z, statistic = Tn3, R = R, sim = "permutation", sizes = sizes, k = k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts >= ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.5),ncol=p);
y <- cbind(rnorm(n2),rnorm(n2));
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12)$p.value
}
alpha <- 0.1;
pow1 <- colMeans(p.values<alpha)
cat(" power of NN =",pow1[1],'\n',"power of energy =",pow1[2],'\n', "power of ball =", pow1[3])
set.seed(123)
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,1,3),ncol=p);
y <- cbind(rnorm(n2,0.5,4),rnorm(n2,0.5,4));
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12)$p.value
}
devtools::build_vignettes()
devtools::build_vignettes()
.Last.error.trace
result<-matrix(nrow = 1, ncol = 4)
result[1,] <- c(0.000,0.000,0.030,0.060)
dimnames(result)[[2]] <- c("n=10","n=20","n=50","n=100")
knitr::kable(result)
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
install.packages('../StatComp21074_1.0.tar.gz',repo=NULL)
library(StatComp21074)
remove.packages("StatComp21074", lib="~/R/win-library/4.1")
devtools::build(vignettes=FALSE)
install.packages('../StatComp_1.0.tar.gz',repo=NULL)
install.packages('../StatComp21074_1.0.tar.gz',repo=NULL)
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
install.packages('../StatComp21074_1.0.tar.gz',repo=NULL)
library(StatComp21074)
remove.packages("StatComp21074", lib="~/R/win-library/4.1")
devtools::document()
devtools::build(vignettes=FALSE)
install.packages('../StatComp_1.0.tar.gz',repo=NULL)
install.packages('../StatComp21074_1.0.tar.gz',repo=NULL)
library(StatComp21074)
remove.packages("StatComp21074", lib="~/R/win-library/4.1")
devtools::build(vignettes=FALSE)
install.packages('../StatComp21074_1.0.tar.gz',repo=NULL)
install.packages('../StatComp_1.0.tar.gz',repo=NULL)
devtools::build(vignettes=FALSE)
install.packages('../StatComp21074_1.0.tar.gz',repo=NULL)
devtools::install_github("zyx1228/statcomp21074",
build_vignettes = TRUE, force=T)
library(StatComp21074)
N = 50  #numbers of nodes
alpha = 1.2  #parameter of power-law
Time = 30   #The length of time
gamma0 = c(-0.5,0.3,0.8,0,0) #fixed parameter of Z
p = length(gamma0)
beta0 = rep(0.3,N) #Intercept term coefficient
Beta = c(0.1,0.5)  #Regression coefficient
Ysim = simudata(beta0, Beta, gamma0, Time, sig = 1, N, alpha)
est(Ysim,N,alpha,p)
